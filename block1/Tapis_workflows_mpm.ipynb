{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tapis v3 Hands-on\n",
    "\n",
    "In this notebook, you will use Tapis v3 to create two systems and an application that will be used to run\n",
    "an MPM job on a HPC like VM.\n",
    "\n",
    "To execute each `In[#]` cell, you can click inside the cell and press `Shift + Enter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Tapipy Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install tapipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter training account information\n",
    "\n",
    "To get things started, please run the following and enter the training account information provided to you. The username and password will be same trainingXX/trainingXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "tenant = 'training'\n",
    "base_url = 'https://' + tenant + '.tapis.io'\n",
    "\n",
    "username = input('Username: ')\n",
    "password = getpass.getpass(prompt='Password: ', stream=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Authenticate and initialize Tapis v3 client\n",
    "\n",
    "Using this information, you can now use `tapipy` to authenticate in the tenant and initialize the\n",
    "Tapis v3 client. You should see your token information displayed. This may take a while to run but should take\n",
    "no more than 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tapipy.tapis import Tapis\n",
    "#Create python Tapis client for user\n",
    "client = Tapis(base_url= base_url, username=username, password=password)\n",
    "# *** Tapis v3: Call to Tokens API\n",
    "client.get_tokens()\n",
    "# Print Tapis v3 token\n",
    "client.access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create Tapis Systems, we need an actual user on the VM. For simplicity we have created the same trainingXX user on the host. The password will be the alphanumeric provided to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "password_vm = getpass.getpass(prompt='Password for VM: ', stream=None)\n",
    "host = input('Host: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Systems\n",
    "\n",
    "In this section we create a Tapis systems, one for running on a VM host using FORK and one for running on an HPC type host using BATCH.\n",
    "\n",
    "Note that although it is possible, we have not provided any login credentials in the system definitions.\n",
    "Well-crafted system definitions are likely to be copied and re-used, so, for security reasons, it is recommended that\n",
    "login credentials be registered using separate API calls as discussed below.\n",
    "\n",
    "### Create a system for the VM host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_id = username\n",
    "system_id_vm = \"tapis-vm-\" + user_id\n",
    "\n",
    "# Create the system definition\n",
    "exec_system_vm = {\n",
    "  \"id\": system_id_vm,\n",
    "  \"description\": \"Test system\",\n",
    "  \"systemType\": \"LINUX\",\n",
    "  \"host\": host,\n",
    "  \"defaultAuthnMethod\": \"PASSWORD\",\n",
    "  \"rootDir\": \"/home/\"+user_id,\n",
    "  \"canExec\": True,\n",
    "  \"jobRuntimes\": [ { \"runtimeType\": \"SINGULARITY\" } ],\n",
    "  \"jobWorkingDir\": \"workdir\",\n",
    "}\n",
    "\n",
    "# Use the client to create the system in Tapis\n",
    "print(\"****************************************************\")\n",
    "print(\"Create system: \" + system_id_vm)\n",
    "print(\"****************************************************\")\n",
    "client.systems.createSystem(**exec_system_vm)\n",
    "\n",
    "\n",
    "# If you need to update the system, you can modify the original definition and use the putSystem call.\n",
    "# - modify the above definition as needed\n",
    "# - comment out the above line with the call to createSystem()\n",
    "# - uncomment the below line with the call to updateSystem()\n",
    "# - re-run the cell\n",
    "# Note that not all attributes may be updated.\n",
    "#client.systems.putSystem(**exec_system_vm, systemId=system_id_vm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You can also update just a few attributes using the patchSystem call.\n",
    "# Note that not all attributes may be updated and some attributes, such as *enabled*,\n",
    "#   may only be updated using a specific call.\n",
    "# For example, to update the description, first define the json to be used:\n",
    "patch_system_vm = {\n",
    "  \"description\": \"System for testing jobs on a VM for Tapis tutorial\"\n",
    "}\n",
    "\n",
    "# Then use the client to make the update:\n",
    "client.systems.patchSystem(**patch_system_vm, systemId=system_id_vm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List all systems available to you\n",
    "print(\"****************************************************\")\n",
    "print(\"List all systems\")\n",
    "print(\"****************************************************\")\n",
    "client.systems.getSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.systems.deleteSystem(systemId='tapis-vm-training1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get details for the system you created\n",
    "print(\"****************************************************\")\n",
    "print(\"Fetch system: \" + system_id_vm)\n",
    "print(\"****************************************************\")\n",
    "client.systems.getSystem(systemId=system_id_vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Register Credentials for the VM system\n",
    "\n",
    "After creating the system, you will need to register credentials for your username. These will be used by Tapis to\n",
    "access the host. Various authentication methods can be used to access a system, such as PASSWORD and PKI_KEYS. For the\n",
    "VM a password is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Register credentials\n",
    "client.systems.createUserCredential(systemId=system_id_vm, userName=user_id, password=password_vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now you can use the client to list files on the system. This will confirm that the credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List files at the rootDir for the system\n",
    "client.files.listFiles(systemId=system_id_vm, path=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create a system for the HPC cluster\n",
    "\n",
    "With just a few changes to the system definition you can create a second system that can be used to run the\n",
    "same application on an HPC type host. Note the minimal changes:\n",
    "\n",
    "* **id** - A unique id is required\n",
    "* **host** - Main hostname for the HPC system.\n",
    "* **rootDir** - Using the root directory of the host gives us flexibility in setting **jobWorkingDir**.\n",
    "  Note that you still need LINUX permissions.\n",
    "* **jobWorkingDir** - Now determined dynamically using the Tapis v3 function HOST_EVAL()\n",
    "* **jobRuntimes** - Most HPC systems support singularity and not docker\n",
    "* **batchLogicalQueue.hpcQueueName** - HPC queue to use by default.\n",
    "* **batchLogicalQueues** - HPC queue definitions for this HPC system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_id = username\n",
    "system_id_hpc = \"tapis-hpc-\" + user_id \n",
    "\n",
    "# Create the system definition\n",
    "exec_system_hpc = {\n",
    "  \"id\": system_id_hpc,\n",
    "  \"description\": \"System for testing jobs on an HPC type host for tapis tutorial\",\n",
    "  \"systemType\": \"LINUX\",\n",
    "  \"host\": host,\n",
    "  \"defaultAuthnMethod\": \"PASSWORD\",\n",
    "  \"rootDir\": \"/home/\"+user_id,\n",
    "  \"canExec\": True,\n",
    "  \"jobRuntimes\": [ { \"runtimeType\": \"SINGULARITY\" } ],\n",
    "  \"jobWorkingDir\": \"workdir\",\n",
    "  \"canRunBatch\": True,\n",
    "  \"batchScheduler\": \"SLURM\",\n",
    "  \"batchSchedulerProfile\": \"tacc\",\n",
    "  \"batchDefaultLogicalQueue\": \"tapisNormal\",\n",
    "  \"batchLogicalQueues\": [\n",
    "    {\n",
    "      \"name\": \"tapisNormal\",\n",
    "      \"hpcQueueName\": \"normal\",\n",
    "      \"maxJobs\": 50,\n",
    "      \"maxJobsPerUser\": 10,\n",
    "      \"minNodeCount\": 1,\n",
    "      \"maxNodeCount\": 16,\n",
    "      \"minCoresPerNode\": 1,\n",
    "      \"maxCoresPerNode\": 68,\n",
    "      \"minMemoryMB\": 1,\n",
    "      \"maxMemoryMB\": 16384,\n",
    "      \"minMinutes\": 1,\n",
    "      \"maxMinutes\": 60\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Use the client to create the system in Tapis\n",
    "print(\"****************************************************\")\n",
    "print(\"Create system: \" + system_id_hpc)\n",
    "print(\"****************************************************\")\n",
    "client.systems.createSystem(**exec_system_hpc)\n",
    "\n",
    "# If you need to update the system,\n",
    "# - modify the above definition as needed\n",
    "# - comment out the above line\n",
    "# - uncomment the below line\n",
    "# - re-run the cell\n",
    "#client.systems.putSystem(**exec_system_hpc, systemId=system_id_hpc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List all systems available to you\n",
    "print(\"****************************************************\")\n",
    "print(\"List all systems\")\n",
    "print(\"****************************************************\")\n",
    "client.systems.getSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get details for the system you created\n",
    "print(\"****************************************************\")\n",
    "print(\"Fetch system: \" + system_id_hpc)\n",
    "print(\"****************************************************\")\n",
    "client.systems.getSystem(systemId=system_id_hpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Register Credentials for the HPC system\n",
    "\n",
    "As before, now you will need to register credentials for your username. These will be used by Tapis to\n",
    "access the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "password_hpc = password_vm\n",
    "# Register credentials\n",
    "client.systems.createUserCredential(systemId=system_id_hpc, userName=user_id, password=password_hpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now you can use the client to list files on the system. This will confirm that the credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List files at the rootDir for the system\n",
    "path_to_list = \"/\"\n",
    "client.files.listFiles(systemId=system_id_hpc, path=path_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Application\n",
    "\n",
    "In order to run a job on a system you will need to create a Tapis application.\n",
    "\n",
    "### Create an application that can be run on the VM host or the HPC cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_id = username\n",
    "app_id = \"mpm-docker-\" + user_id\n",
    "\n",
    "# Create the application definition\n",
    "app_def = {\n",
    "    \"id\": app_id,\n",
    "    \"version\": \"dev\",\n",
    "    \"jobType\": \"FORK\",\n",
    "    \"runtime\": \"DOCKER\",\n",
    "    \"description\": \"High-Performance Material Point Method (CB-Geo mpm) DEVELOPMENT version.\",\n",
    "    \"containerImage\": \"tapis/mpm:dev\",\n",
    "    \"jobAttributes\": {\n",
    "        \"isMpi\": False,\n",
    "        \"parameterSet\": {\n",
    "            \"appArgs\": [\n",
    "                {\"name\": \"directoryInputFlag\", \"arg\": \"-f\", \"inputMode\": \"FIXED\"},\n",
    "                {\"name\": \"directoryInput\", \"arg\": \"/home/cbgeo/research/mpm-benchmarks/2d/uniaxial_stress/\", \"inputMode\": \"REQUIRED\"}\n",
    "            ] \n",
    "        },\n",
    "        \"fileInputs\": [\n",
    "            {\n",
    "                \"name\": \"directoryInput\",\n",
    "                \"inputMode\": \"OPTIONAL\",\n",
    "                \"targetPath\": \".\",\n",
    "                \"description\": \"Input directory that contains the MPM congiguration file as well as any other required files. Note that to utilize this attribute one must also set the directoryInput parameter to mbe the value of the name of the directory. Also note that if this directory is not provided, a default (included in the appliation container image) will be used.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "# Use the client to create the application in Tapis\n",
    "print(\"****************************************************\")\n",
    "print(\"Create application: \" + app_id)\n",
    "print(\"****************************************************\")\n",
    "client.apps.createAppVersion(**app_def)\n",
    "\n",
    "# If you need to update the application,\n",
    "# - modify the above definition as needed\n",
    "# - comment out the above line\n",
    "# - uncomment the below line\n",
    "# - re-run the cell\n",
    "#client.apps.putApp(**app_def, appId=app_id, appVersion=\"0.0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List all applications available to you\n",
    "print(\"****************************************************\")\n",
    "print(\"List all applications\")\n",
    "print(\"****************************************************\")\n",
    "client.apps.getApps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get details for the application you created\n",
    "print(\"****************************************************\")\n",
    "print(\"Fetch application: \" + app_id)\n",
    "print(\"****************************************************\")\n",
    "client.apps.getAppLatestVersion(appId=app_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Jobs\n",
    "\n",
    "We will run two jobs, one on the VM host using FORK and one on the HPC type host using BATCH.\n",
    "\n",
    "We will use the same Tapis application to run both jobs.\n",
    "\n",
    "### Part 1: Run Material Point Method (MPM) app on a Virtual Machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run MPM app on a Virtual Machine\n",
    "\n",
    "# Submit a job\n",
    "job_response_vm=client.jobs.submitJob(name='mpm-job-vm',description='material point method',appId=app_id,execSystemId=system_id_vm,appVersion= 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Job submission response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get Job submission response\n",
    "print(\"****************************************************\")\n",
    "print(\"Job Submitted: \" + app_id)\n",
    "print(\"****************************************************\")\n",
    "print(job_response_vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Jobs Listings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get Jobs listings\n",
    "client.jobs.getJobList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Job UUID from the submission response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get job uuid from the job submission response\n",
    "print(\"****************************************************\")\n",
    "job_uuid_vm=job_response_vm.uuid\n",
    "print(\"Job UUID: \" + job_uuid_vm)\n",
    "print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check the status of the job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the status of the job\n",
    "print(\"****************************************************\")\n",
    "print(client.jobs.getJobStatus(jobUuid=job_uuid_vm))\n",
    "print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download output of the job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Once the job is in the FINISHED state, you can download output of the job\n",
    "print(\"Job Output file:\")\n",
    "\n",
    "print(\"****************************************************\")\n",
    "jobs_output_vm= client.jobs.getJobOutputDownload(jobUuid=job_uuid_vm,outputPath='stdout')\n",
    "print(jobs_output_vm)\n",
    "print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cancel a job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If necessary, you can cancel a long running job.\n",
    "# To cancel a running job\n",
    "# client.jobs.cancelJob(jobUuid=job_uuid_vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Run a Batch Job on HPC type host\n",
    "\n",
    "Using the same Tapis application we can also run the image classifier as a batch job on an HPC type host\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run MPM app on the HPC Machine\n",
    "\n",
    "# Submit a job\n",
    "job_response_hpc=client.jobs.submitJob(name='mpm-hpc',description='mpm',appId=app_id,execSystemId=system_id_hpc,appVersion= 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Job submission response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"****************************************************\")\n",
    "print(\"Job Submitted: \" + app_id)\n",
    "print(\"****************************************************\")\n",
    "print(job_response_hpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check job status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the status of the job\n",
    "print(\"****************************************************\")\n",
    "job_uuid_hpc=job_response_hpc.uuid\n",
    "print(client.jobs.getJobStatus(jobUuid=job_uuid_hpc))\n",
    "print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download output of the HPC job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download output of the job\n",
    "print(\"Job Output file:\")\n",
    "\n",
    "print(\"****************************************************\")\n",
    "jobs_output_hpc= client.jobs.getJobOutputDownload(jobUuid=job_uuid_hpc,outputPath='stdout')\n",
    "print(jobs_output_hpc)\n",
    "print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflows\n",
    "\n",
    "In this section, we are going to use tapipy to construct a pipeline that builds and HPC application container image, pushes it to a remote image registry, then run some tests in a container using the HPC application "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerhub Credentials\n",
    "\n",
    "First we need to set our Dockerhub credentials. This will be used to give the image builder permissions to push to your Dockerhub account.\n",
    "\n",
    "#### NOTE:\n",
    "Your Dockerhub credentials will be encrypted and safely stored in the Tapis Security Kernel (backed by HasiCorp Vault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerhub_username = input('Dockerhub username: ')\n",
    "dockerhub_personal_access_token = getpass.getpass(prompt='Dockerhub Access Token: ', stream=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Group\n",
    "All workflow resources must exist within a group. A group is collection of users that have access to workflow resources such as Pipelines and Tasks. Anyone that belongs to a group can create their own pipelines and run pipelines owned by that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the group\n",
    "group_id = \"pearc23-group-\" + username\n",
    "\n",
    "print(\"****************************************************\")\n",
    "create_group_resp = client.workflows.createGroup(id=group_id)\n",
    "print(create_group_resp)\n",
    "print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pipeline\n",
    "Pipelines are simply collections of tasks. Tasks can be added to a pipeline after it is created or directly in the pipeline definition itself. For this demonstration we will be creating everything at once.\n",
    "\n",
    "The first task in this pipeline is an image build task. Image build tasks require a \"context\", which is the source control repository which contains the Dockerfile we want to build from.\n",
    "\n",
    "The next two tasks run jobs on an HPC system to ensure that there are no errors with the image. The first test ensures that MPM was compiled correctly and the second run a test script called uniaxial traction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the group\n",
    "pipeline_id = \"pearc23-pipeline-\" + username\n",
    "\n",
    "print(\"****************************************************\")\n",
    "create_pipeline_resp = client.workflows.createPipeline(**{\n",
    "    \"id\": pipeline_id,\n",
    "    \"group_id\": group_id,\n",
    "    \"type\": \"workflow\",\n",
    "    \"execution_profile\": {\n",
    "        \"max_retries\": 0,\n",
    "        \"invocation_mode\": \"async\",\n",
    "        \"duplicate_submission_policy\": \"terminate\", # Terminates the current running pipeline if another is submitted\n",
    "        \"max_exec_time\": 3600 # in seconds\n",
    "    },\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"id\": \"build-mpm-image\",\n",
    "            \"pipeline_id\": pipeline_id,\n",
    "            \"group_id\": group_id,\n",
    "            \"type\": \"image_build\",\n",
    "            \"builder\": \"kaniko\", # Alternative to docker that allows you to build containers in containers\n",
    "            \"context\": {\n",
    "                \"type\": \"github\",\n",
    "                \"branch\": \"main\",\n",
    "                \"url\": \"tapis-project/application-repository\",\n",
    "                \"build_file_path\": \"Dockerfile\",\n",
    "                \"sub_path\": \"/material-point-method/mpm-dummy-src/docker_build\",\n",
    "                \"visibility\": \"public\"\n",
    "            },\n",
    "            \"destination\": {\n",
    "                \"type\": \"dockerhub\",\n",
    "                \"url\": f\"{dockerhub_username}/dummy-mpm\",\n",
    "                \"tag\": \"pearc-test\",\n",
    "                \"credentials\": {\n",
    "                    \"username\": dockerhub_username,\n",
    "                    \"token\": dockerhub_personal_access_token\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"test-mpm-compiled\",\n",
    "            \"type\": \"tapis_job\",\n",
    "            \"tapis_job_def\": {\n",
    "                \"name\": 'mpm-compiled-correctly',\n",
    "                \"description\": 'material point method',\n",
    "                \"appId\": app_id,\n",
    "                \"execSystemId\": system_id_vm,\n",
    "                \"appVersion\": 'dev'\n",
    "            },\n",
    "            \"depends_on\": [\n",
    "                {\"id\": \"build-mpm-image\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"test-mpm-uniaxial-traction\",\n",
    "            \"type\": \"tapis_job\",\n",
    "            \"tapis_job_def\": {\n",
    "                \"name\": \"mpm-uniaxial-traction-test\",\n",
    "                \"appId\": app_id,\n",
    "                \"appVersion\": \"dev\",\n",
    "                \"execSystemId\": system_id_vm,\n",
    "                \"appArgs\": {\n",
    "                    \"directoryInput\": \"./benchmarks/2d/uniaxial_traction/\"\n",
    "                }\n",
    "            },\n",
    "            \"depends_on\": [\n",
    "                {\"id\": \"build-mpm-image\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "print(create_pipeline_resp)\n",
    "print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a pipeline\n",
    "\n",
    "Once a pipeline has been definined it can now be run with a simple call the runPipeline endpoint.\n",
    "\n",
    "#### NOTE:\n",
    "\n",
    "In our execution profile of our pipeline definition, we set the execution profile to `terminate` this means that once you run a pipeline, all subsequent submissions of a pipeline to workflow engine will terminate the one previously running, so only run the cell below a single time or the next run of the pipeline may end up delayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"****************************************************\")\n",
    "runs = client.workflows.listPipelineRuns(group_id=group_id, pipeline_id=pipeline_id)\n",
    "sorted_runs = sorted(runs, key=lambda run: run.started_at, reverse=True)\n",
    "if (\n",
    "    len(sorted_runs) == 0 \n",
    "    or (len(sorted_runs) > 0 and sorted_runs[0].status not in [\"pending\", \"active\"])\n",
    "):\n",
    "    run_pipeline_resp = client.workflows.runPipeline(group_id=group_id, pipeline_id=pipeline_id)\n",
    "    print(run_pipeline_resp)\n",
    "else:\n",
    "    print(f\"Pipeline currently {sorted_runs[0].status}\")\n",
    "print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the PipelineRun Status\n",
    "\n",
    "Your pipeline is now running. It will take some time for the HPC image to build. In the meantime, you can check the status of the run by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_runs = client.workflows.listPipelineRuns(group_id=group_id, pipeline_id=pipeline_id)\n",
    "last = sorted(pipeline_runs, key=lambda run: run.started_at, reverse=True)[0]\n",
    "task_executions = client.workflows.listTaskExecutions(group_id=group_id, pipeline_id=pipeline_id, pipeline_run_uuid=last.uuid)\n",
    "print(\"****************************************************\")\n",
    "print(f\"Current Pipeline Run - {last.status}\")\n",
    "print(\"****************************************************\")\n",
    "print(f\"Task Executions [{len(task_executions)}]\")\n",
    "print(\"****************************************************\")\n",
    "for i, execution in enumerate(sorted(task_executions, key=lambda ex: ex.started_at)):\n",
    "    print(f\"[{i}]\", execution.task_id, execution.status)\n",
    "print(\"****************************************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
